#+TITLE: TypedFlow: The HOT parts
#+AUTHOR: Jean-Philippe Bernardy, University of Gothenburg

TensorFlow™ is an open source software library for numerical
computation using data flow graphs. Nodes in the graph represent
mathematical operations, while the graph edges represent the
multidimensional data arrays (tensors) communicated between them.
TensorFlow graphs can be efficiently evaluated on GPUs and is a
popular choice to implement deep learning applications.

TypedFlow is a higher-order and typed (HOT) frontend to tensorflow,
written in (Glasgow) Haskell.

 In this talk I will:
   - briefly explain what TensorFlow is and how it applies to deep learning
   - recall the advantages of a HOT approach
   - expose some example programs written using TypedFlow
   - demonstrate how tensorflow functions can be given precise types,
     using GHC extensions
   - discuss some the difficulties of doing so

* Machine learning in 45 seconds:

- a vector of training inputs X :: [A]
- a model f : (Θ × A) → ℝ⁺

Task:

Given X, find θ such that f(θ,x) < ε, if
x is considered similar to points in X.

Commentary: Every point in X lie on a manyfold. We want to find what
this manyfold is. (Interpolation problem.)

* "Deep" learning in 45 seconds

- "Deep" ≡ f is "complex"
- So we must use a brute force method to compute θ: stochastic
  gradient descent (or variants thereof).

- Typically, compute the gradient of f wrt. θ using AD.

* Tensorflow

- A (meta) programming language to define f. AD is builtin. (there is fineprint)

- Restricted control flow (mostly, tensor-generalisations of +, *, -,
  /, ^)

- Typically programmed using python (standard in scientific computing)

- "Strongly typed".
  - but: "brodcasting"
  - but: running the metaprogram can be quite slow ~1 minute (so type
    errors can happen after 1 minute of loading the model --- and
    programs can do other things before ...)
  - but: types are typically not written as such. Given any two
    functions, weather they compose (and what the composition does) is
    a mistery unless one examines their code.

* First-order culture

https://github.com/fchollet/keras/blob/master/keras/layers/recurrent.py
(search "class LSTM")

* TypedFlow

- An typed, higher-order frontend to tensorflow (basic tensor operations)
- A library to construct neural networks
- Generates python (yikes)

* Typing tensors

- tanh
- matmul
- concatT
- repeatT
- tile
- convolution

* Heterogeneous tensors

type HTV

* Example Higher-order stuff

- mapT
- rnn
- withBypass
- Attention model

* Complete examples

- mnist
- seq2seq

* GHC woes

- see transposeV

* Summary

- Some NN building blocks are naturally higher-order. Taking an
  example (and simplifying) a recurrent neural network turns a tensor
  function into a function between lists (vectorslists) of tensors.

- Functional programming is ideally suited to program complicated
  applications from building blocks.

  Example: an "Attention-model" is a thing where every step in a RNN adds
  a computation which depends on an external input. We can compose
  usual RNN cells with attention models in several ways. The state of
  the art is to reprogram all combinations by hand.

- Typed APIs.

  Types can be used to check the tensor dimensions. Types catch a lof
  of errors, but they can also be used to *guide* the programming.

  Types are pretty much a necessity in the presence of HO
  functions.

- TypedFlow is typically much closer to mathematical notation than
  python. Programs are short to write and easier to read. Standard
  building blocks can be swapped for custom versions quite easily.

  Examples
    - rnn stacking using "residual connections" instead of just
      stacking.
    - make it easy to share parameters between different components
      (example: if we do a style translation we may want to share the
      embedding layers between encoder and decoders parts)

- Long game: integrate cutting edge ideas as they arrive with moderate
  effort.

* FAQ
- Why not Agda, Idris?  A long term plan is to bypass python, so we'd
  want a "real" programming language for the programming bits that go
  around the TF program.
